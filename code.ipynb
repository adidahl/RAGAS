{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adidahl/RAGAS/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQG4iSxVxw8f"
      },
      "source": [
        "# RAGAS Evaluation for LangChain Agents (Using ChromaDB and OpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eOnr6z_zLoc",
        "outputId": "6d2620c2-3d67-440d-a38f-f4089566a461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov6TCS7bx1oI"
      },
      "source": [
        "**R**etrieval **A**ugmented **G**eneration **As**sessment (RAGAS) is an evaluation framework for quantifying the performances of our RAG pipelines. In this example we will see how to use it with a RAG-enabled conversational agent in LangChain, using **ChromaDB** as the vector store and **OpenAI** for models.\n",
        "\n",
        "Because we need an agent and RAG pipeline to evaluate RAGAS the first part of this notebook covers setting up an XML Agent with RAG. Jump ahead to **Integrating RAGAS** for the RAGAS section.\n",
        "\n",
        "To begin, let's install the prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zshhLDrgbFKk",
        "outputId": "39c006a7-ba60-43b9-d7c7-9b35f7ddffac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-openai \\\n",
        "    openai \\\n",
        "    chromadb==0.5.0 \\\n",
        "    tiktoken==0.7.0 \\\n",
        "    datasets==2.19.0 \\\n",
        "    ragas==0.1.8 \\\n",
        "    pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VeosBMaIDs52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a9fcd2-b302-43b4-bd13-6c7dcd20eeb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Run this cell and enter your OpenAI API Key when prompted\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# platform.openai.com\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "  openai_api_key = getpass(\"Please enter your OpenAI API key: \")\n",
        "  os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "  print(\"OpenAI API Key already set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpKfZkUYzQhB"
      },
      "source": [
        "## Finding Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDTQoxcNzUa8"
      },
      "source": [
        "The first thing we need for an agent using RAG is somewhere we want to pull knowledge from. We will use v2 of the AI ArXiv dataset, available on Hugging Face Datasets at [`jamescalam/ai-arxiv2-chunks`](https://huggingface.co/datasets/jamescalam/ai-arxiv2-chunks).\n",
        "\n",
        "_Note: we're using the prechunked dataset. For the raw version see [`jamescalam/ai-arxiv2`](https://huggingface.co/datasets/jamescalam/ai-arxiv2)._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9gpYFnzbFKm",
        "outputId": "b3972241-e3c8-4c9a-8870-26baab441c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
              "    num_rows: 5000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a smaller subset for quicker testing, adjust as needed\n",
        "dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:5000]\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bP7ZW-ybFKm",
        "outputId": "3a2de650-a9db-40f4-fd81-e85ca1ebddf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doi': '2401.09350',\n",
              " 'chunk-id': 1,\n",
              " 'chunk': 'These neural networks and their training algorithms may be complex, and the scope of their impact broad and wide, but nonetheless they are simply functions in a high-dimensional space. A trained neural network takes a vector as input, crunches and transforms it in various ways, and produces another vector, often in some other space. An image may thereby be turned into a vector, a song into a sequence of vectors, and a social network as a structured collection of vectors. It seems as though much of human knowledge, or at least what is expressed as text, audio, image, and video, has a vector representation in one form or another.\\nIt should be noted that representing data as vectors is not unique to neural networks and deep learning. In fact, long before learnt vector representations of pieces of dataâ\\x80\\x94what is commonly known as â\\x80\\x9cembeddingsâ\\x80\\x9dâ\\x80\\x94came along, data was often encoded as hand-crafted feature vectors. Each feature quanti- fied into continuous or discrete values some facet of the data that was deemed relevant to a particular task (such as classification or regression). Vectors of that form, too, reflect our understanding of a real-world object or concept.',\n",
              " 'id': '2401.09350#1',\n",
              " 'title': 'Foundations of Vector Retrieval',\n",
              " 'summary': 'Vectors are universal mathematical objects that can represent text, images,\\nspeech, or a mix of these data modalities. That happens regardless of whether\\ndata is represented by hand-crafted features or learnt embeddings. Collect a\\nlarge enough quantity of such vectors and the question of retrieval becomes\\nurgently relevant: Finding vectors that are more similar to a query vector.\\nThis monograph is concerned with the question above and covers fundamental\\nconcepts along with advanced data structures and algorithms for vector\\nretrieval. In doing so, it recaps this fascinating topic and lowers barriers of\\nentry into this rich area of research.',\n",
              " 'source': 'http://arxiv.org/pdf/2401.09350',\n",
              " 'authors': 'Sebastian Bruch',\n",
              " 'categories': 'cs.DS, cs.IR',\n",
              " 'comment': None,\n",
              " 'journal_ref': None,\n",
              " 'primary_category': 'cs.DS',\n",
              " 'published': '20240117',\n",
              " 'updated': '20240117',\n",
              " 'references': []}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX6NdQhgbFKn"
      },
      "source": [
        "## Building the Knowledge Base with ChromaDB and OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCbqQl_bFKn"
      },
      "source": [
        "To build our knowledge base we need _two things_:\n",
        "\n",
        "1.  **Embeddings:** We will use `OpenAIEmbeddings` which requires the OpenAI API key we provided earlier.\n",
        "2.  **A vector database:** We will use `ChromaDB`, a popular open-source vector database that can run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wkw0KyLRbFKo"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "import os\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "# Using a smaller, efficient model like text-embedding-3-small is recommended\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhDzfsczbFKo"
      },
      "source": [
        "Now, let's set up ChromaDB. We'll use a persistent client so the data isn't lost when the notebook session ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "chroma_setup_cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386e8222-4e1e-4a59-911d-e5aa2031e413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-300f37dba083>:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma vector store initialized.\n",
            "Collection name: arxiv_openai_ragas\n",
            "Persisting to: ./chroma_db_arxiv\n",
            "Number of documents currently in collection: 0\n"
          ]
        }
      ],
      "source": [
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Define the path for the persistent Chroma database\n",
        "persist_directory = \"./chroma_db_arxiv\"\n",
        "# Define the collection name\n",
        "collection_name = \"arxiv_openai_ragas\"\n",
        "\n",
        "# Initialize Chroma vector store. This will create the directory if it doesn't exist\n",
        "# or load it if it does.\n",
        "vectorstore = Chroma(\n",
        "    collection_name=collection_name,\n",
        "    embedding_function=embed,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(f\"Chroma vector store initialized.\")\n",
        "print(f\"Collection name: {collection_name}\")\n",
        "print(f\"Persisting to: {persist_directory}\")\n",
        "print(f\"Number of documents currently in collection: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Ep3743bFKo"
      },
      "source": [
        "Let's check the dimensionality of our OpenAI embedding model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwMhLWLDbFKo",
        "outputId": "9e88964b-8983-4b1c-9b04-32d5bce0f801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dimension: 1536\n"
          ]
        }
      ],
      "source": [
        "sample_embedding = embed.embed_query(\"test query\")\n",
        "embedding_dim = len(sample_embedding)\n",
        "print(f\"Embedding dimension: {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZUn2lu7bFKp"
      },
      "source": [
        "### Populating our ChromaDB Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeVD6d0sbFKp"
      },
      "source": [
        "Now our knowledge base is ready to be populated with our data. We will use the `vectorstore.add_texts` method, which handles embedding generation internally.\n",
        "\n",
        "We will include metadata from each record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "3623a05370914c089a36c3147e53c7a1",
            "df87f3de1eaf4e74889fcd46382d76ca",
            "99e44f9a57824f798e647e745a192017",
            "faff7d40b97e49bea3ca6d36e282182a",
            "31a85ef3a3834ffbb45edbc75a140272",
            "a48acee8582948009668a91d350094d4",
            "711ad01a1b514ca8882c7540dbb585c3",
            "a1d566e5420347f988d2b06cd9e65c55",
            "b18655d8e93b4fe3bac404999e850988",
            "bf6fd756b40f4ef59c5524b5f764d20f",
            "90d3c212fa7c4941ab2c58d198acdeab"
          ]
        },
        "id": "hb00VSTqbFKp",
        "outputId": "85715fac-a2b2-45ec-b4e2-4c2a4679eed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing to add 5000 documents to ChromaDB in batches of 100...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3623a05370914c089a36c3147e53c7a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finished adding documents.\n",
            "Total documents added in this run: 5000\n",
            "Total documents in collection: 5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-cf7ecea610c7>:34: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# easier to work with dataset as pandas dataframe\n",
        "data = dataset.to_pandas()\n",
        "\n",
        "batch_size = 100 # ChromaDB can handle larger batches, adjust as needed\n",
        "total_docs = len(data)\n",
        "docs_added = 0\n",
        "\n",
        "print(f\"Preparing to add {total_docs} documents to ChromaDB in batches of {batch_size}...\")\n",
        "\n",
        "for i in tqdm(range(0, total_docs, batch_size)):\n",
        "    i_end = min(total_docs, i+batch_size)\n",
        "    # get batch of data\n",
        "    batch = data.iloc[i:i_end]\n",
        "    # generate unique ids for each chunk\n",
        "    ids = batch[\"id\"].tolist()\n",
        "    # get text to embed (which becomes the document content)\n",
        "    texts = batch['chunk'].tolist()\n",
        "    # get metadata to store\n",
        "    metadata = batch[['source', 'title']].to_dict('records')\n",
        "\n",
        "    # Add to ChromaDB\n",
        "    try:\n",
        "        vectorstore.add_texts(texts=texts, metadatas=metadata, ids=ids)\n",
        "        docs_added += len(ids)\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding batch {i//batch_size + 1}: {e}\")\n",
        "        # Optional: break or continue depending on desired behavior\n",
        "        # break\n",
        "\n",
        "# Persist the changes explicitly (good practice)\n",
        "vectorstore.persist()\n",
        "print(f\"\\nFinished adding documents.\")\n",
        "print(f\"Total documents added in this run: {docs_added}\")\n",
        "print(f\"Total documents in collection: {vectorstore._collection.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6VVT3X_EMDO"
      },
      "source": [
        "Create a tool for our agent to use when searching for ArXiv papers in our ChromaDB collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X9J5jHKcEQz6"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "\n",
        "@tool\n",
        "def arxiv_search(query: str) -> str:\n",
        "    \"\"\"Use this tool when answering questions about AI, machine learning, data\n",
        "    science, or other technical questions that may be answered using arXiv\n",
        "    papers.\n",
        "    \"\"\"\n",
        "    # Perform similarity search in ChromaDB\n",
        "    # We use the vectorstore object directly, which already has the embedder\n",
        "    docs = vectorstore.similarity_search(query, k=5)\n",
        "    # Reformat results into string (Langchain docs have 'page_content')\n",
        "    # RAGAS expects a list of strings as context, so we return the combined string here\n",
        "    # which will be split later during RAGAS evaluation.\n",
        "    results_str = \"\\n---\\n\".join(\n",
        "        [doc.page_content for doc in docs]\n",
        "    )\n",
        "    if not results_str:\n",
        "        return \"No relevant documents found.\"\n",
        "    return results_str\n",
        "\n",
        "tools = [arxiv_search]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN7d_4r-JMPW"
      },
      "source": [
        "When this tool is used by our agent it will execute it like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq4H-2RpI1U3",
        "outputId": "e3d26014-1e5f-44eb-88e3-9d14795c3aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\n",
            "---\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n",
            "# Mixtral 8x7B\n",
            "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# 3.1 Multilingual benchmarks\n",
            "---\n",
            "0.644 0.622 0.632 0.690 0.794 0.740 0.808 0.046 0.473 0.247 0.589 0.653 0.491 0.486 0.532 0.394 0.396 0.515 0.725 0.686 0.728 0.767 0.742 0.054 0.500 0.399 0.669 0.571 0.701 0.705 0.672 0.294 0.521 0.428 0.590 0.427 0.601 0.575 0.573 0.135 0.696 0.732 0.738 0.758 0.776 0.710 0.735 0.321 0.658 0.877 0.777 0.876 0.857 0.888 0.870 0.207 0.541 0.533 0.681 0.662 0.692 0.689 0.687 0.177 0.525 0.546 0.616 0.620 0.637 0.642 0.673 Llama2-chat-7B Llama2-chat-70B Llama2-chat-13B Vicuna-V1.3-7B WizardLM LongChat-V1-13B LongChat-V1.5-7B LongChat-V1-7B\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    arxiv_search.run(tool_input={\"query\": \"can you tell me about llama 2?\"})\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUvJOqrNhYIh"
      },
      "source": [
        "## Defining XML Agent with OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s45dwd78hbvk"
      },
      "source": [
        "The XML agent is built primarily to support Anthropic models. Anthropic models have been trained to use XML tags like `<input>{some input}</input` or when using a tool they use:\n",
        "\n",
        "```\n",
        "<tool>{tool name}</tool>\n",
        "<tool_input>{tool input}</tool_input>\n",
        "```\n",
        "\n",
        "While OpenAI models are more commonly used with ReAct or Function Calling agents, we will proceed with the XML agent as requested, using an OpenAI model (`gpt-3.5-turbo`). Its ability to strictly follow the XML format might vary.\n",
        "\n",
        "To create an XML agent we need a `prompt`, `llm`, and list of `tools`. We can download a prebuilt prompt for conversational XML agents from LangChain hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntuT7UuXeMz0",
        "outputId": "96870afc-78c6-47af-bb97-ef64e0c5ff63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tools'], input_types={}, partial_variables={'chat_history': ''}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'xml-agent-convo', 'lc_hub_commit_hash': '00f6b7470fa25a24eef6e4e3c1e44ba07189f3e91c4d987223ad232490673be8'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'], input_types={}, partial_variables={}, template=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nPrevious Conversation:\\n{chat_history}\\n\\nQuestion: {input}\\n{agent_scratchpad}\"), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfdcKCdwi0SL"
      },
      "source": [
        "We can see the XML format being used throughout the prompt when explaining to the LLM how it should use tools. Now we initialize the OpenAI LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kDHuU2uOdW91"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# chat completion llm using OpenAI\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    model_name='gpt-4o', # Or 'gpt-4-turbo-preview', 'gpt-4' etc.\n",
        "    temperature=0.0 # Low temperature for more deterministic factual answers\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g33Nt-xijPKG"
      },
      "source": [
        "When the agent is run we will provide it with a single `input` — this is the input text from a user. However, within the agent logic an *agent_scratchpad* object will be passed too, which will include tool information. To feed this information into our LLM we will need to transform it into the XML format described above, we define the `convert_intermediate_steps` function to handle that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TMMBgMBlIJoq"
      },
      "outputs": [],
      "source": [
        "def convert_intermediate_steps(intermediate_steps):\n",
        "    log = \"\"\n",
        "    for action, observation in intermediate_steps:\n",
        "        log += (\n",
        "            f\"<tool>{action.tool}</tool><tool_input>{action.tool_input}\"\n",
        "            f\"</tool_input><observation>{observation}</observation>\"\n",
        "        )\n",
        "    return log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_PQWVckAOi"
      },
      "source": [
        "We must also parse the tools into a string containing `tool_name: tool_description` — we handle that with the `convert_tools` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qxbrF5a4j9il"
      },
      "outputs": [],
      "source": [
        "def convert_tools(tools):\n",
        "    return \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCVI2dyUIRg6"
      },
      "source": [
        "With everything ready we can go ahead and initialize our agent object using [**L**ang**C**hain **E**xpression **L**anguage (LCEL)](https://python.langchain.com/docs/expression_language/). We add instructions for when the LLM should _stop_ generating with `llm.bind(stop=[...])` and finally we parse the output from the agent using an `XMLAgentOutputParser` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z3yhTDmEIU4n"
      },
      "outputs": [],
      "source": [
        "from langchain.agents.output_parsers.xml import XMLAgentOutputParser\n",
        "\n",
        "agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        # Ensure chat_history is passed for conversational context\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        \"agent_scratchpad\": lambda x: convert_intermediate_steps(\n",
        "            x[\"intermediate_steps\"]\n",
        "        ),\n",
        "    }\n",
        "    | prompt.partial(tools=convert_tools(tools))\n",
        "    | llm.bind(stop=[\"</tool_input>\", \"</observation>\"]) # Adjusted stop sequences for robustness\n",
        "    | XMLAgentOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG2_hL4hkudq"
      },
      "source": [
        "With our `agent` object initialized we pass it to an `AgentExecutor` object alongside our original `tools` list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YHW_K3WOIsXw"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "# Define memory key consistent with the prompt\n",
        "memory_key = \"chat_history\"\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key=memory_key, return_messages=True)\n",
        "\n",
        "# Initialize AgentExecutor with agent, tools, memory, and intermediate steps\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True, # Set to True to see agent reasoning\n",
        "    return_intermediate_steps=True,\n",
        "    handle_parsing_errors=True # Helps with potential XML parsing issues\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCtHauRlkLc"
      },
      "source": [
        "Now we can use the agent via the `invoke` method. Note that we need to provide an empty `chat_history` for the first turn if using memory this way directly (although the memory object handles this)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Aqp20qloj7",
        "outputId": "1084a94a-1778-40ea-ed61-64c9b409c42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>llama 2\u001b[0m\u001b[36;1m\u001b[1;3mLlama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\n",
            "---\n",
            "0.644 0.622 0.632 0.690 0.794 0.740 0.808 0.046 0.473 0.247 0.589 0.653 0.491 0.486 0.532 0.394 0.396 0.515 0.725 0.686 0.728 0.767 0.742 0.054 0.500 0.399 0.669 0.571 0.701 0.705 0.672 0.294 0.521 0.428 0.590 0.427 0.601 0.575 0.573 0.135 0.696 0.732 0.738 0.758 0.776 0.710 0.735 0.321 0.658 0.877 0.777 0.876 0.857 0.888 0.870 0.207 0.541 0.533 0.681 0.662 0.692 0.689 0.687 0.177 0.525 0.546 0.616 0.620 0.637 0.642 0.673 Llama2-chat-7B Llama2-chat-70B Llama2-chat-13B Vicuna-V1.3-7B WizardLM LongChat-V1-13B LongChat-V1.5-7B LongChat-V1-7B\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\n",
            "---\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n",
            "# Mixtral 8x7B\n",
            "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# 3.1 Multilingual benchmarks\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models with different parameter sizes, such as 7B, 13B, and 70B. Llama 2 models are designed to perform well across various benchmarks, including reasoning, comprehension, and STEM reasoning. They are compared with other models like Mistral 7B and GPT-3.5, with Mistral 7B often outperforming Llama 2 in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated on multilingual benchmarks and are noted for their efficiency in the cost-performance spectrum.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'can you tell me about llama 2?', 'chat_history': [HumanMessage(content='can you tell me about llama 2?', additional_kwargs={}, response_metadata={}), AIMessage(content='Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models with different parameter sizes, such as 7B, 13B, and 70B. Llama 2 models are designed to perform well across various benchmarks, including reasoning, comprehension, and STEM reasoning. They are compared with other models like Mistral 7B and GPT-3.5, with Mistral 7B often outperforming Llama 2 in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated on multilingual benchmarks and are noted for their efficiency in the cost-performance spectrum.', additional_kwargs={}, response_metadata={})], 'output': 'Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models with different parameter sizes, such as 7B, 13B, and 70B. Llama 2 models are designed to perform well across various benchmarks, including reasoning, comprehension, and STEM reasoning. They are compared with other models like Mistral 7B and GPT-3.5, with Mistral 7B often outperforming Llama 2 in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated on multilingual benchmarks and are noted for their efficiency in the cost-performance spectrum.', 'intermediate_steps': [(AgentAction(tool='arxiv_search', tool_input='llama 2', log='<tool>arxiv_search</tool><tool_input>llama 2'), 'Llama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â\\x9c\\x93 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\\n---\\n0.644 0.622 0.632 0.690 0.794 0.740 0.808 0.046 0.473 0.247 0.589 0.653 0.491 0.486 0.532 0.394 0.396 0.515 0.725 0.686 0.728 0.767 0.742 0.054 0.500 0.399 0.669 0.571 0.701 0.705 0.672 0.294 0.521 0.428 0.590 0.427 0.601 0.575 0.573 0.135 0.696 0.732 0.738 0.758 0.776 0.710 0.735 0.321 0.658 0.877 0.777 0.876 0.857 0.888 0.870 0.207 0.541 0.533 0.681 0.662 0.692 0.689 0.687 0.177 0.525 0.546 0.616 0.620 0.637 0.642 0.673 Llama2-chat-7B Llama2-chat-70B Llama2-chat-13B Vicuna-V1.3-7B WizardLM LongChat-V1-13B LongChat-V1.5-7B LongChat-V1-7B\\n---\\nDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\\n---\\nTable 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\nSize and Efficiency. We computed â\\x80\\x9cequivalent model sizesâ\\x80\\x9d of the Llama 2 family, aiming to understand Mistral 7B modelsâ\\x80\\x99 efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâ\\x80\\x99s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\\n# Instruction Finetuning\\n---\\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n4\\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\\n# Mixtral 8x7B\\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\\n# 3.1 Multilingual benchmarks')]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import traceback\n",
        "\n",
        "try:\n",
        "    response = agent_executor.invoke({\n",
        "        \"input\": \"can you tell me about llama 2?\",\n",
        "        \"chat_history\": [] # Start with empty history for invoke\n",
        "    })\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"Agent invocation failed: {e}\")\n",
        "    print(\"--- Full Traceback ---\")\n",
        "    traceback.print_exc() # Print the detailed traceback\n",
        "    print(\"----------------------\")\n",
        "    response = {'output': f'Agent failed to generate response: {e}', 'intermediate_steps': []}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0L_80WrpWqd"
      },
      "source": [
        "Now let's put together a helper function `chat` to simplify interaction, using the agent's memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C-Ck2Lv53rD-"
      },
      "outputs": [],
      "source": [
        "def chat(text: str):\n",
        "    # The AgentExecutor now manages chat history via the memory object\n",
        "    try:\n",
        "        out = agent_executor.invoke({\"input\": text})\n",
        "    except Exception as e:\n",
        "        print(f\"Chat invocation failed: {e}\")\n",
        "        out = {'output': 'Agent failed to generate response.', 'intermediate_steps': []}\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIheLeTBsO9S"
      },
      "source": [
        "Now we simply chat with our agent and it will remember the context of previous interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ_PH7YcA_f2",
        "outputId": "c455cd7b-b880-48c6-f424-7a4790be1b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>llama 2\u001b[0m\u001b[36;1m\u001b[1;3mLlama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\n",
            "---\n",
            "0.644 0.622 0.632 0.690 0.794 0.740 0.808 0.046 0.473 0.247 0.589 0.653 0.491 0.486 0.532 0.394 0.396 0.515 0.725 0.686 0.728 0.767 0.742 0.054 0.500 0.399 0.669 0.571 0.701 0.705 0.672 0.294 0.521 0.428 0.590 0.427 0.601 0.575 0.573 0.135 0.696 0.732 0.738 0.758 0.776 0.710 0.735 0.321 0.658 0.877 0.777 0.876 0.857 0.888 0.870 0.207 0.541 0.533 0.681 0.662 0.692 0.689 0.687 0.177 0.525 0.546 0.616 0.620 0.637 0.642 0.673 Llama2-chat-7B Llama2-chat-70B Llama2-chat-13B Vicuna-V1.3-7B WizardLM LongChat-V1-13B LongChat-V1.5-7B LongChat-V1-7B\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\n",
            "---\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n",
            "# Mixtral 8x7B\n",
            "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# 3.1 Multilingual benchmarks\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models of various sizes, such as 7B, 13B, and 70B parameters. Llama 2 models are designed to perform well across a range of tasks, including reasoning, comprehension, and code generation. They have been compared to other models like Mistral 7B and GPT-3.5, with Llama 2 showing competitive performance on several benchmarks. However, Mistral 7B has been noted to outperform Llama 2 13B in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated for their efficiency in terms of cost-performance, with some models achieving performance levels expected from larger models.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "User: can you tell me about llama 2?\n",
            "Agent: Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models of various sizes, such as 7B, 13B, and 70B parameters. Llama 2 models are designed to perform well across a range of tasks, including reasoning, comprehension, and code generation. They have been compared to other models like Mistral 7B and GPT-3.5, with Llama 2 showing competitive performance on several benchmarks. However, Mistral 7B has been noted to outperform Llama 2 13B in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated for their efficiency in terms of cost-performance, with some models achieving performance levels expected from larger models.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Reset memory before starting a new conversation if needed\n",
        "agent_executor.memory.clear()\n",
        "\n",
        "response1 = chat(\"can you tell me about llama 2?\")\n",
        "print(f'\\nUser: can you tell me about llama 2?')\n",
        "print(f'Agent: {response1.get(\"output\", \"Error: No output found.\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p8m4Gc5w1OX"
      },
      "source": [
        "We can ask follow up questions that miss key information but thanks to the conversational history the LLM understands the context and uses that to adjust the search query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XJ_3JIgBDRl",
        "outputId": "8324cf38-7a8b-42fe-ab82-ca88135045b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>Llama 2 red teaming\u001b[0m\u001b[36;1m\u001b[1;3mLlama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\n",
            "---\n",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\n",
            "Figure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\n",
            "We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n",
            "# Instruction Fine-tuning\n",
            "---\n",
            "TogetherCompute. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.\n",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Yes, red teaming was conducted for Llama 2. Red teaming is a process where a model is tested for vulnerabilities, biases, and other potential issues by simulating adversarial attacks or challenging scenarios. This helps in identifying and mitigating risks associated with the deployment of the model. The details of the red teaming process for Llama 2 are likely documented in the technical papers or reports associated with its development.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "User: was any red teaming done?\n",
            "Agent: Yes, red teaming was conducted for Llama 2. Red teaming is a process where a model is tested for vulnerabilities, biases, and other potential issues by simulating adversarial attacks or challenging scenarios. This helps in identifying and mitigating risks associated with the deployment of the model. The details of the red teaming process for Llama 2 are likely documented in the technical papers or reports associated with its development.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Ask a follow-up question\n",
        "response2 = chat(\"was any red teaming done?\")\n",
        "print(f'\\nUser: was any red teaming done?')\n",
        "print(f'Agent: {response2.get(\"output\", \"Error: No output found.\")}')\n",
        "\n",
        "# Store the last response with intermediate steps for RAGAS\n",
        "last_response_with_steps = response2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bI9czPtWnl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNHpkMPoDKEV"
      },
      "source": [
        "## Integrating RAGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuyybSe5FlZg"
      },
      "source": [
        "To integrate RAGAS evaluation into this pipeline we need a few things, from our pipeline we need the **query**, the **retrieved contexts**, and the **generated output**.\n",
        "\n",
        "We have the generated output (`answer`) and the query (`question`). We need to extract the retrieved contexts from the agent's intermediate steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPJkvJh1DJSD",
        "outputId": "64317008-c9f9-4855-b39d-9635644e11c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'was any red teaming done?', 'chat_history': [HumanMessage(content='can you tell me about llama 2?', additional_kwargs={}, response_metadata={}), AIMessage(content='Llama 2 is a family of language models developed as an improvement over the original Llama models. It includes models of various sizes, such as 7B, 13B, and 70B parameters. Llama 2 models are designed to perform well across a range of tasks, including reasoning, comprehension, and code generation. They have been compared to other models like Mistral 7B and GPT-3.5, with Llama 2 showing competitive performance on several benchmarks. However, Mistral 7B has been noted to outperform Llama 2 13B in certain areas like code, mathematics, and reasoning benchmarks. Llama 2 models are also evaluated for their efficiency in terms of cost-performance, with some models achieving performance levels expected from larger models.', additional_kwargs={}, response_metadata={}), HumanMessage(content='was any red teaming done?', additional_kwargs={}, response_metadata={}), AIMessage(content='Yes, red teaming was conducted for Llama 2. Red teaming is a process where a model is tested for vulnerabilities, biases, and other potential issues by simulating adversarial attacks or challenging scenarios. This helps in identifying and mitigating risks associated with the deployment of the model. The details of the red teaming process for Llama 2 are likely documented in the technical papers or reports associated with its development.', additional_kwargs={}, response_metadata={})], 'output': 'Yes, red teaming was conducted for Llama 2. Red teaming is a process where a model is tested for vulnerabilities, biases, and other potential issues by simulating adversarial attacks or challenging scenarios. This helps in identifying and mitigating risks associated with the deployment of the model. The details of the red teaming process for Llama 2 are likely documented in the technical papers or reports associated with its development.', 'intermediate_steps': [(AgentAction(tool='arxiv_search', tool_input='Llama 2 red teaming', log='<tool>arxiv_search</tool><tool_input>Llama 2 red teaming'), 'Llama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â\\x9c\\x93 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng et al. 2023) Vicuna-V1.5 (Zheng et al. 2023) WizardLM (Xu et al. 2023b) LongChat-V1 (Li* et al. 2023) LongChat-V1.5 (Li* et al. 2023) OpenChat-V3.2 (Wang et al. 2023a) GPT-3.5-turbo GPT-4 Llama2 Llama1 Llama2 Llama1 Llama1 Llama2 Llama2 - - 7B, 13B, 70B 7B, 13B, 33B 7B, 13B 13B 7B, 13B 7B 13B - - N/A N/A N/A N/A N/A N/A N/A N/A N/A 4k 2k 16k 2k 16k 32k\\n---\\nLlama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\\n# Instruction Fine-tuning\\n---\\nTogetherCompute. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\\n---\\nDetailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\\n4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\\n3\\njm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\\n---\\nTable 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\\nSize and Efficiency. We computed â\\x80\\x9cequivalent model sizesâ\\x80\\x9d of the Llama 2 family, aiming to understand Mistral 7B modelsâ\\x80\\x99 efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâ\\x80\\x99s performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\\n# Instruction Finetuning')]}\n"
          ]
        }
      ],
      "source": [
        "# Display the last response which includes intermediate steps\n",
        "print(last_response_with_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjFyIBvFLcIo"
      },
      "source": [
        "When initializing our `AgentExecutor` object we included `return_intermediate_steps=True`. Those steps include the response from our `arxiv_search` tool — which we can use the evaluate the retrieval portion of our pipeline with RAGAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwCU9B1rOkZ9"
      },
      "source": [
        "We extract the contexts themselves like so (remembering the tool output is a single string joined by `\\n---\\n`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He3I1jAGOnI9",
        "outputId": "7c946f79-9418-4a86-ce82-c001b46c9443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5 contexts:\n",
            "--- Context 1 ---\n",
            "Llama2 7B, 13B 7B 7B, 13B, 33B 13B 7B 7B 7B 13B 2k 1k 8k 2k 4k 4k 4k 4k 5w 200w 200w, 300w, 430w 110w 1000w â 120w 100w English-oriented Models Llama2-chat (Touvron et al. 2023) Vicuna-V1.3 (Zheng e...\n",
            "--- Context 2 ---\n",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0...\n",
            "--- Context 3 ---\n",
            "TogetherCompute. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.\n",
            "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavi...\n",
            "--- Context 4 ---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categorie...\n",
            "--- Context 5 ---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchm...\n"
          ]
        }
      ],
      "source": [
        "contexts = []\n",
        "intermediate_steps = last_response_with_steps.get('intermediate_steps', [])\n",
        "\n",
        "if intermediate_steps:\n",
        "    # Assuming the arxiv_search tool is the first (and likely only) step\n",
        "    tool_output = intermediate_steps[0][1] # Output of the tool (observation)\n",
        "    if isinstance(tool_output, str):\n",
        "        contexts = tool_output.split(\"\\n---\\n\")\n",
        "    else:\n",
        "        print(\"Tool output is not a string, cannot extract contexts.\")\n",
        "else:\n",
        "    print(\"No intermediate steps found to extract contexts from.\")\n",
        "\n",
        "print(f\"Extracted {len(contexts)} contexts:\")\n",
        "for i, ctx in enumerate(contexts):\n",
        "    print(f\"--- Context {i+1} ---\")\n",
        "    print(ctx[:200] + \"...\") # Print start of each context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuhMvGdBOnvu"
      },
      "source": [
        "## Evaluation with RAGAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bczg8jREMwLw"
      },
      "source": [
        "To evaluate with RAGAS we need an evaluation dataset containing `question`s, and the `ground_truth` answers to those questions. RAGAS can then use the `answer` generated by our agent and the retrieved `contexts` to calculate various metrics.\n",
        "\n",
        "We will use a pre-made evaluation dataset based on the AI ArXiv dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "cdb5700136604edbb41a38f12d1bec63",
            "271b2957ee434acb854a06a83eaf5774",
            "08e3bd7f9c6d45e0aebc27b6991885c5",
            "ef08c2d004c74161859d9f0dfa35fc2a",
            "48d65d19d08e4daaba34a9e1b36e43a4",
            "8dc87c8b8386458cb2439b1a8ee062f6",
            "c1f752c6be944633892fcfdad69dfb33",
            "adc75b37ff4748a1aa5e88acd338d9f3",
            "0a4f061dd14046188b1ec72f1b74e0a1",
            "ee2ef68d735f4223aad2a06da4c7e570",
            "3c15f9da62e341c58878cb5911db1a02",
            "d72f9242f54a44f6a1b9414ae1803164",
            "e417f4be46e84539bfd0ac3d69c99f14",
            "8abfb84c3c004f27b6b8882da70c95b2",
            "ae362487109c4859ae84adf133c74286",
            "65f2330c47b74ce08cb9495afe5750e8",
            "21b4679f227a405fb6da3e684a254d8a",
            "a0ce5d063a814981bd5e46c4fde742c5",
            "c0f869c3677f44f79262fb6370e55f54",
            "ec90b8dd36764eb3baa580089e57eeb4",
            "6012890e8fab4fa1884095fdd77d6334",
            "6e9ac41d57084446ae1c364ac12bf1d7"
          ]
        },
        "id": "Ee-LezlRGSsu",
        "outputId": "5bb0dc01-759b-49d1-80be-fc754861fa13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/87.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb5700136604edbb41a38f12d1bec63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/51 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d72f9242f54a44f6a1b9414ae1803164"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'ground_truth_context', 'ground_truth', 'question_type', 'episode_done'],\n",
              "    num_rows: 51\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Using the mixtral dataset, but evaluation uses OpenAI\n",
        "ragas_data = load_dataset(\"aurelio-ai/ai-arxiv2-ragas-mixtral\", split=\"train\")\n",
        "# Note: This dataset includes ground_truth_context, which RAGAS v0.1.x\n",
        "# can use for context_recall/precision if available, but often they are calculated\n",
        "# without pre-defined relevant contexts, comparing generated contexts to ground_truth answer.\n",
        "ragas_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsSjPe0MNRi1",
        "outputId": "d5a9a8c1-6244-43a4-fc96-f650cd08a351"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is the impact of encoding the input prompt on inference speed in generative inference?',\n",
              " 'ground_truth_context': ['- This technique works particularly well when processing large batches of data, during train-\\ning Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al.\\n(2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded\\nfrom RAM.\\n- In turn, when doing interactive inference (e.g. as a chat assistants), offloading works\\nsignificantly slower than on-device inference.\\n- The generative inference workload consists of two phases: 1) encoding the input prompt and 2)\\ngenerating tokens conditioned on that prompt.\\n- The key difference between these two phases is that\\nprompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially\\n(token-by-token and layer-by-layer).\\n- In general, phase 1 works relatively well with existing Mixture-\\nof-Experts algorithms, since each layer can only be loaded once for the entire prompt.\\n- In turn, when\\ngenerating tokens, one must load layer once per each token generated.\\n- In practice, this means that\\ninference speed is limited by how fast one can fetch parameters from system memory.\\n- Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit\\nthese patterns to speed up inference time.\\n- As we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to\\nassign individual experts to distinct sub-tasks.\\n- However, this does not mean that the model uses the\\nsame expert over long stretches of tokens.\\n- Instead, some experts are active in short sequences of 2-4\\ntokens, while others are often used with “gaps”, as shown in Figure 1.\\n- To take advantage of this pattern, we can keep active experts in GPU memory as a “cache” for\\nfuture tokens.\\n- If the same experts are activated again in future, they will be available instantaneously.\\n- While LRU caching can reduce the average expert loading time, most of the inference time is still\\nspent waiting for the next expert to be loaded.\\n- The reason behind this is that, unlike with dense\\nmodels, MoE offloading cannot effectively overlap expert loading with computation.\\n- For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads\\nthe next transformer layer ahead of time, while the previous layer is still running.\\n- Unfortunately,\\nthis schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose\\nwhich experts to load just-in-time for computation.'],\n",
              " 'ground_truth': ['The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively well with existing Mixture-of-Experts algorithms. Each layer only needs to be loaded once for the entire prompt. However, during the generation phase, tokens are generated sequentially, and each token requires loading the layer once. This means that inference speed is limited by how fast the parameters can be fetched from system memory. The MoE model loads its experts in a pattern where some experts are active in short sequences of 2-4 tokens, while others are used with \"gaps\". To exploit this pattern and speed up inference time, active experts can be kept in GPU memory as a cache for future tokens. If the same experts are activated again in the future, they will be available instantaneously. However, even with caching, most of the inference time is still spent waiting for the next expert to be loaded because MoE offloading cannot effectively overlap expert loading with computation like dense models can.'],\n",
              " 'question_type': 'conditional',\n",
              " 'episode_done': False}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "ragas_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU4QrqiPOF-w"
      },
      "source": [
        "We first iterate through the questions in this evaluation dataset and ask these questions to our agent, collecting the questions, answers, contexts, and ground truths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2252b8845604422883461382ffade0bc",
            "c0e179f760d842ebb0f6d25bd9727e3d",
            "9d704bd907474277aed5eb9924d843e4",
            "2b6e2621bb6a42acb091ba16815dce7a",
            "cd6e575240d74588b88966eb1d546aa2",
            "c06219f55b5d4656bcadf081c42373b2",
            "068c25497bbd42d0a604c86b9744c03b",
            "7acd4a6ad3a7442ea4f319146ed0da99",
            "bba2f654afc74096bc03711db8e6244d",
            "7c181b9310fa4ef39a999d53bb44cd33",
            "6f230e7d6d33472ba9666edcccdc1c5c"
          ]
        },
        "id": "rfK4Y2kGNUUD",
        "outputId": "ecd57ab4-e88e-456f-c992-43b0994a83e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for 10 questions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2252b8845604422883461382ffade0bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>impact of encoding input prompt on inference speed in generative models\u001b[0m\u001b[36;1m\u001b[1;3mThe generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n",
            "# 2.4 Hardware Setup\n",
            "---\n",
            "Prompting is central to the downstream performance of foundation models. For example, different prompt strategies1 can have a significant impact on a modelâs reasoning abilities (Wei et al., 2022; Nye et al., 2021; Zhou et al., 2022; Wang et al., 2022; Zhou et al., 2023; Wang et al., 2023b), multi- modal processing abilities (Yang et al., 2023b; Wang et al., 2023d), or tool use abilities (Yao et al., 2022; Schick et al., 2023). Furthermore, prompting can improve model distillation (Wang et al., 2023c; Hsieh et al., 2023) and it can be used to simulate agentic behavior (Wang et al., 2023a; Park et al., 2023; Wu et al., 2023). However, these prompt strategies are manually engineered. Since the specific way a prompt is phrased can have a dramatic effect on its utility (Madaan & Yazdanbakhsh, 2022), it raises the question of whether prompt engineering can be automated. Automatic Prompt Engineer (APE, Zhou et al., 2023) attempts to address this by generating an initial distribution of prompts\n",
            "---\n",
            "# A TECHNICAL DETAILS A.1 Implementation of Soft+Hard Prompting\n",
            "To implement the soft+hard prompting strategy discussed in Section 3.3.2 for decoder-only LLMs such as GPT, we can generate only the \"keys\" and \"values\" for the heterogeneous tokens in the prompts ð,ð x , and use the \"query\" of the last token as a start to generate ð ð,ð the homogeneous tokens of the main texts x for language ð modeling. For encoder-decoder-based LLMs such as T5, a natural ð¢ð£,ð ð,ð thought is to input the prompts x in the encoder, and use ð ð ð ð,ð , x the decoder to generate the main texts x ð\n",
            "# A.2 Recommendation-Oriented Finetuning\n",
            "If we denote the multinomial probability obtained from the Re- cLLM prediction head ðððð as Ërâððð , and denote the stacked item\n",
            "---\n",
            "Designing the seed improver. Our chosen seed improver (Figure 2) simply prompts the language model to generate candidate improvements of an initial solution and then returns the best solution according to the utility function. We chose this particularly simple form to provide nontrivial improvement for a generic downstream task while 1) encouraging the language model to be as âcreativeâ as possible, 2) minimizing initial prompt complexity, since self-improvement introduces additional complexity due to nested references to code strings inside of prompts, and 3) minimizing the prompt token count and therefore the costs of querying a language model. We also considered other variants of this seed prompt but heuristically found that this version maximized the novelty of improver improvements proposed by the GPT-4 language model.\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>The impact of encoding the input prompt on inference speed in generative models is significant due to the difference in processing phases. Encoding the input prompt is done in parallel, layer-by-layer, which is generally efficient and works well with existing Mixture-of-Experts (MoE) algorithms. However, the generation phase, which is sequential (token-by-token and layer-by-layer), is slower because it requires loading each layer for every token generated. This sequential nature limits inference speed, as it depends on how quickly parameters can be fetched from system memory. Therefore, while encoding the input prompt is relatively fast, the overall inference speed is often constrained by the slower token generation phase.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>generating tokens affect inference speed in generative inference\u001b[0m\u001b[36;1m\u001b[1;3mâ¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from a token generation perspective. Instead of focusing on the number of tokens in the prompt, it takes into account the number of tokens generated. This is particularly significant because transformer-based generative LLMs produce content token-by-token, with each subsequent token relying on the gen- eration of preceding ones. Consequently, an increase in number of generated tokens leads to a corresponding increase in the computational cost, as each additional generated token implies another LLM forward inference. In fact, OpenAI applies a pricing structure wherein the cost for the number of generated tokens is twice that of the number of prompt tokens for their LLM APIs 1. This underscores the substantial impact that generated tokens can have on computational expenses.\n",
            "---\n",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "# 2.4 Hardware Setup\n",
            "While our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n",
            "# 3 Method\n",
            "In this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.\n",
            "---\n",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n",
            "# 2.4 Hardware Setup\n",
            "---\n",
            "sort algorithm, regardless of the model size. Overall, the variety of results we observe across different experimental settings shows the importance of not drawing conclusions about effectiveness from single datasets or model sizes.\n",
            "5.2 Efficiency Results Regarding computational and runtime efficiency, the results pre- sented in Table 2 indicate that both Pointwise methods exhibit fewest inference, prompt tokens, and no generated tokens. Furthermore, their computational efficiency and query latency are optimized due to efficient GPU-based batched inference. It is worth noting, how- ever, that these methods do come with certain limitations. Specifi- cally, they require access to the model output logits (thus currently limiting their use to just open source LLMs) and are less effective when used with larger models. In contrast, pairwise.allpair appears to be the most expensive method that consumes the most number of prompt tokens and generated tokens due to the large number of document pair preferences needed to be inferred. Hence, even with GPU batching, pairwise.allpair still has the worst query latency. In constrast, approaches utilizing our Setwise promptingânamely, list- wise.likelihood, setwise.heapsort, and setwise.bubblesort, are far more efficient than their counterparts, listwise.generate, pairwise.heapsort, and pairwise.bubblesort respectively. Notably, these improvements\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating tokens is done sequentially, where each token depends on the previously generated ones. This sequential process increases computational cost and time, as each additional token requires a forward pass through the model. Consequently, the inference speed is limited by how quickly model parameters can be fetched from memory, making the process slower compared to parallel processing. This impact is particularly pronounced in interactive settings, such as chat applications, where tokens are generated one at a time.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mTo answer your question about the differences in architecture between Mixtral 8x7B and Mistral 7B, particularly in terms of feedforward blocks and active parameters used during inference, I will search for relevant information in arXiv papers.\n",
            "\n",
            "<tool>arxiv_search</tool><tool_input>Mixtral 8x7B architecture vs Mistral 7B feedforward blocks active parameters inference\u001b[0m\u001b[36;1m\u001b[1;3mAbstract\n",
            "We introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B â Instruct, that surpasses Llama 2 13B â chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
            "# Introduction\n",
            "---\n",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\n",
            "---\n",
            "Abstract\n",
            "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\n",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n",
            "---\n",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n",
            "# 2 Architectural details\n",
            "Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value\n",
            "---\n",
            "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP, or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B â Chat model.\n",
            "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications.\n",
            "# 2 Architectural details\n",
            "The cat sat on the The cat sat on the window size â_ââ> The cat sat on the Vanilla Attention Sliding Window Attention Effective Context Length\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>The architecture of Mixtral 8x7B differs from Mistral 7B primarily in its use of Sparse Mixture of Experts (SMoE) layers. While Mistral 7B is a standard transformer model, Mixtral 8x7B incorporates 8 feedforward blocks (experts) per layer. For each token, a router network selects two experts to process the token, allowing each token to access 47 billion parameters but only using 13 billion active parameters during inference. This approach enables Mixtral 8x7B to use fewer active parameters compared to Mistral 7B, which does not employ the mixture of experts architecture.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>offloading A100 server MoE-based language models\u001b[0m\u001b[36;1m\u001b[1;3m# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n",
            "# Abstract\n",
            "With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n",
            "# Introduction\n",
            "---\n",
            "Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. ModelScope-Agent: Building your customizable agent system with open-source large language models. arXiv preprint arXiv:2309.00986, 2023a.\n",
            "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for âmindâ exploration of large scale language model society. arXiv preprint arXiv:2303.17760, 2023b.\n",
            "Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023c.\n",
            "---\n",
            "Game: Quanti- fying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Sun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu, X.; Shao, Y.; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y.; Zhou, Z.; Li, R.; Zhan, J.; Zhou, Y.; Li, L.; Yang, X.; Wu, L.; Yin, Z.; Huang, X.; and Qiu, X. 2023a. MOSS: Training Conver- sational Language Models from Synthetic Data. Sun, W.; Yan, L.; Ma, X.; Ren, P.; Yin, D.; and Ren, Z. 2023b. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. arXiv preprint arXiv:2304.09542. Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan- ford alpaca: An instruction-following llama model. Team, I. 2023. InternLM: A Multilingual Language Model with\n",
            "---\n",
            "generating, and translating human language with an unprecedented degree of accuracy and sophistication. However, the proliferation of these models has also been accompanied by a growing trend towards commercialization and a lack of transparency, a phenomenon that is increasingly influencing the dynamics of the open-source community.\n",
            "Historically, the open-source community has thrived on the principles of collaboration, trans- parency, and unrestricted sharing of ideas. However, as the commercial potential of LLMs has been recognized, this openness has begun to diminish. The reality is that many organi- zations only make model checkpoints publicly accessible, while withholding vital information on model reproduction. This practice signifi- cantly hampers the progress of the field.\n",
            "In an effort to revive the spirit of the open- source community and contribute to the on- going dialogue about transparency in AI, we present Skywork-13B: a family of bilingual large language models with 13 billion parameters, trained on a colossal corpus of more than 3.2 trillion tokens drawn from both English and Chinese texts. To our knowledge, our Skywork- 13B is the most thoroughly trained family of open LLMs of comparable size to date.\n",
            "1\n",
            "# Introduction\n",
            "---\n",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\n",
            "In this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\n",
            "we observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\n",
            "â¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Offloading is used on the A100 server for accelerating MoE-based language models to manage the large model size and limited accelerator memory. The strategy involves using parameter offloading algorithms that take advantage of the innate properties of MoE language models. Specifically, it uses a MoE-specific offloading strategy that leverages regularities in how the model accesses its experts, such as reusing experts between adjacent tokens and predicting which experts are needed ahead of time. This approach reduces GPU-RAM communication and overlaps expert loading with computation, allowing for faster token generation even on consumer-grade hardware with limited memory.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>Mixtral Llama 2 70B code benchmarks comparison\u001b[0m\u001b[36;1m\u001b[1;3mTable 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\n",
            "---\n",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
            "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
            "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.\n",
            "---\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n",
            "# Mixtral 8x7B\n",
            "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# 3.1 Multilingual benchmarks\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Mixtral significantly outperforms Llama 2 70B in code benchmarks, as well as in most other categories, while using 5x fewer active parameters during inference. This makes Mixtral not only more efficient but also superior in performance, particularly in code and mathematics benchmarks. However, Llama 2 70B performs better in reading comprehension benchmarks. Overall, Mixtral is a more efficient model in terms of active parameter usage and offers superior performance in code-related tasks.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>Mixtral mathematics benchmarks Llama 2 70B\u001b[0m\u001b[36;1m\u001b[1;3mTable 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n",
            "70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\n",
            "Figure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.\n",
            "---\n",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "4\n",
            "LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n",
            "# Mixtral 8x7B\n",
            "Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# 3.1 Multilingual benchmarks\n",
            "---\n",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\n",
            "Size and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\n",
            "Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.\n",
            "---\n",
            "Detailed results for Mistral 7B, Llama 2 7B/13B, and Code-Llama 7B are reported in Table 2. Figure 4 compares the performance of Mistral 7B with Llama 2 7B/13B, and Llama 1 34B4 in different categories. Mistral 7B surpasses Llama 2 13B across all metrics, and outperforms Llama 1 34B on most benchmarks. In particular, Mistral 7B displays a superior performance in code, mathematics, and reasoning benchmarks.\n",
            "4Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n",
            "3\n",
            "jm Mistral 7B = mm LLaMA2 138 50 lm Mistral 7B mm LLaMA2 138 mmm LlaMA278 lm LLaMA1 348 bel mmm LlaMA2 78 mem LlaMA 1348 70 40 vt = = eo g 7 = 330 Â£ g gs0 : < <20 40 10 ay MMLU Knowledge Reasoning Comprehension AGI Eval Math BBH Code\n",
            "---\n",
            "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and approaches the code performance of Code-Llama 7B without sacrificing performance on non-code benchmarks.\n",
            "Size and Efficiency. We computed âequivalent model sizesâ of the Llama 2 family, aiming to understand Mistral 7B modelsâ efficiency in the cost-performance spectrum (see Figure 5). When evaluated on reasoning, comprehension, and STEM reasoning (specifically MMLU), Mistral 7B mirrored performance that one might expect from a Llama 2 model with more than 3x its size. On the Knowledge benchmarks, Mistral 7Bâs performance achieves a lower compression rate of 1.9x, which is likely due to its limited parameter count that restricts the amount of knowledge it can store.\n",
            "Evaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n",
            "# Instruction Finetuning\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>Mixtral outperforms Llama 2 70B on mathematics benchmarks. It achieves superior performance while using 5 times fewer active parameters during inference. This efficiency is due to Mixtral's sparse Mixture-of-Experts model, which uses only 13 billion active parameters per token compared to Llama 2 70B's full parameter set. Overall, Mixtral demonstrates better performance across most categories, including mathematics, compared to Llama 2 70B.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>Mixtral-8x7B-Instruct model OpenAssistant dataset benchmarking expert LRU cache speculative loading expert recall rate\u001b[0m\u001b[36;1m\u001b[1;3mFor this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n",
            "# 4.2 Mixed MoE Quantization\n",
            "---\n",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n",
            "5\n",
            "iy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\n",
            "Figure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n",
            "# 4.1 Expert LRU Cache and Speculative Loading\n",
            "In this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.\n",
            "---\n",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\n",
            "Many consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.\n",
            "---\n",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n",
            "# 3.2 Speculative Expert Loading\n",
            "While LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.\n",
            "---\n",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n",
            "6\n",
            "Algorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>The relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset involves evaluating the effectiveness of these two expert offloading strategies. The expert recall rate, or \"hit ratio,\" measures how often an expert needed for inference is already available on the GPU. \n",
            "\n",
            "1. **LRU Caching**: This strategy involves caching a fixed number of experts per layer. The recall rate is measured by the cache hit ratio for different cache sizes. The LRU cache helps in reducing the average expert loading time by keeping frequently used experts in the cache, thus improving the recall rate.\n",
            "\n",
            "2. **Speculative Loading**: This strategy involves pre-loading experts based on predictions of which experts will be needed in the next layers. The recall rate is measured by how often the speculatively loaded experts are actually used. Speculative loading is tested by pre-loading experts 1, 2, or 10 layers ahead, and the recall is measured accordingly.\n",
            "\n",
            "Both strategies aim to improve the efficiency of the Mixtral-8x7B model by ensuring that the necessary experts are available on the GPU when needed, thus reducing inference time and improving performance on the OpenAssistant dataset. The effectiveness of these strategies is benchmarked by measuring the expert recall rate under different configurations.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>sparse Mixture-of-Experts language models faster token generation\u001b[0m\u001b[36;1m\u001b[1;3mThe generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n",
            "# 2.2 Post-training Quantization of LLMs\n",
            "---\n",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).\n",
            "---\n",
            "# 2 Background & Related Work\n",
            "# 2.1 Mixture-of-Experts\n",
            "The recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.\n",
            "---\n",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n",
            "[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n",
            "[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n",
            "[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>The use of sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation primarily by leveraging a gating function to select only a subset of experts for processing each token. This approach reduces the computational load compared to dense models, as not all parameters are used for every token. During the token generation phase, which is sequential, the MoE model can efficiently load only the necessary parameters for the selected experts, thus speeding up the inference process. This efficiency is particularly beneficial when generating tokens, as it minimizes the time spent fetching parameters from system memory, which is a bottleneck in dense models. Additionally, MoE models are generally cheaper to train and perform inference, although they require more parameters than dense models with equivalent perplexity.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>sparse Mixture-of-Experts impact on language model size\u001b[0m\u001b[36;1m\u001b[1;3mShazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).\n",
            "---\n",
            "# 2 Background & Related Work\n",
            "# 2.1 Mixture-of-Experts\n",
            "The recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.\n",
            "---\n",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n",
            "# 2.2 Post-training Quantization of LLMs\n",
            "---\n",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\n",
            "---\n",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n",
            "[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n",
            "[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n",
            "[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>The use of sparse Mixture-of-Experts (MoE) in language models impacts their size by allowing them to have a large number of parameters while maintaining computational efficiency. Sparse MoE models consist of multiple \"experts\" and a gating function that selects a small subset of these experts for each input, which means that not all parameters are used simultaneously. This results in models that are cheaper to train and perform inference compared to dense models with equivalent perplexity. However, MoE models typically require more parameters than dense models, leading to very large model sizes, often over a trillion parameters. Despite their size, they can achieve near state-of-the-art performance, as seen with models like Mixtral-8x7B. The large size of MoE models can make them difficult to use without high-end hardware, which may limit their adoption compared to dense models.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m<tool>arxiv_search</tool><tool_input>LRU caching Mixture-of-Experts language models inference speed\u001b[0m\u001b[36;1m\u001b[1;3mWe illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n",
            "# 3.2 Speculative Expert Loading\n",
            "While LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.\n",
            "---\n",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\n",
            "Below, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n",
            "4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n",
            "3\n",
            "Selected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture\n",
            "---\n",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n",
            "# 4.2 Mixed MoE Quantization\n",
            "---\n",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n",
            "# 3.1 Expert Locality and LRU caching\n",
            "As we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\n",
            "To take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.\n",
            "---\n",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n",
            "# Abstract\n",
            "With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n",
            "# Introduction\u001b[0m\u001b[32;1m\u001b[1;3m<final_answer>LRU (Least Recently Used) caching improves the inference speed of Mixture-of-Experts (MoE) language models by storing active experts in GPU memory as a cache for future tokens. This caching strategy takes advantage of the pattern where some experts are frequently activated in short sequences of tokens. By keeping the least recently used experts in memory, the model can quickly access them if they are activated again, reducing the time spent loading experts from system memory. This approach is particularly beneficial during the token generation phase, where each layer must be loaded for every token generated. Although LRU caching is a simple strategy, it can significantly speed up inference by reducing the average expert loading time, even though most inference time is still spent waiting for the next expert to be loaded.</final_answer>\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Finished collecting evaluation data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain/memory/chat_memory.py:55: UserWarning: 'ConversationBufferMemory' got multiple output keys: dict_keys(['output', 'intermediate_steps']). The default 'output' key is being used. If this is not desired, please manually set 'output_key'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "eval_results = []\n",
        "limit = 10 # Number of questions to evaluate (adjust as needed)\n",
        "\n",
        "print(f\"Running evaluation for {limit} questions...\")\n",
        "\n",
        "# Reset agent memory before evaluation loop\n",
        "agent_executor.memory.clear()\n",
        "\n",
        "for i, row in tqdm(enumerate(ragas_data), total=limit):\n",
        "    if i >= limit:\n",
        "        break\n",
        "    question = row[\"question\"]\n",
        "    # RAGAS expects ground_truths as a list of strings\n",
        "    ground_truths = [row[\"ground_truth\"]] if isinstance(row[\"ground_truth\"], str) else row[\"ground_truth\"]\n",
        "\n",
        "    # Clear memory for each new question to avoid contamination between eval samples\n",
        "    agent_executor.memory.clear()\n",
        "\n",
        "    try:\n",
        "        # Use invoke to get intermediate steps\n",
        "        out = agent_executor.invoke({\"input\": question, \"chat_history\": []})\n",
        "        answer = out.get('output', \"ERROR: No output\")\n",
        "        intermediate_steps = out.get('intermediate_steps', [])\n",
        "\n",
        "        contexts = []\n",
        "        if intermediate_steps:\n",
        "            # Assuming the search tool is the first step\n",
        "            tool_output = intermediate_steps[0][1]\n",
        "            if isinstance(tool_output, str):\n",
        "                 contexts = tool_output.split(\"\\n---\\n\")\n",
        "            else:\n",
        "                 print(f\"Warning: Tool output for Q{i} is not a string.\")\n",
        "        else:\n",
        "             # This happens if the agent answers without using the tool\n",
        "             print(f\"Warning: No intermediate steps (tool not used) for Q{i}. Contexts will be empty.\")\n",
        "             contexts = []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question {i}: {e}\")\n",
        "        answer = f\"ERROR: {e}\"\n",
        "        contexts = []\n",
        "\n",
        "    eval_results.append({\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": contexts,\n",
        "        \"ground_truth\": ground_truths[0] # RAGAS expects single string ground_truth\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(eval_results)\n",
        "print(\"\\nFinished collecting evaluation data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "QrpnyBfuj0uL",
        "outputId": "d34ce5b8-26e9-4f00-824c-86eba30d6242"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "0  The impact of encoding the input prompt on inference speed in generative models is significant due to the difference in processing phases. Encoding the input prompt is done in parallel, layer-by-l...   \n",
              "1  Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating ...   \n",
              "2  The architecture of Mixtral 8x7B differs from Mistral 7B primarily in its use of Sparse Mixture of Experts (SMoE) layers. While Mistral 7B is a standard transformer model, Mixtral 8x7B incorporate...   \n",
              "3  Offloading is used on the A100 server for accelerating MoE-based language models to manage the large model size and limited accelerator memory. The strategy involves using parameter offloading alg...   \n",
              "4  Mixtral significantly outperforms Llama 2 70B in code benchmarks, as well as in most other categories, while using 5x fewer active parameters during inference. This makes Mixtral not only more eff...   \n",
              "\n",
              "                                                                                                                                                                                                  contexts  \\\n",
              "0  [The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that pro...   \n",
              "1  [â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from...   \n",
              "2  [Abstract\\nWe introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all e...   \n",
              "3  [# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\\n# Abstract\\nWith the widespread adoption of Large Language Models (LLMs), many deep learning pr...   \n",
              "4  [Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mix...   \n",
              "\n",
              "                                                                                                                                                                                              ground_truth  \n",
              "0  The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively ...  \n",
              "1  Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference sy...  \n",
              "2  The architecture of Mixtral 8x7B differs from Mistral 7B in terms of feedforward blocks and active parameters used during inference. Mixtral 8x7B has 8 feedforward blocks (experts) in each layer, ...  \n",
              "3  Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for r...  \n",
              "4                                                                                                                                                      Mixtral outperforms Llama 2 70B in code benchmarks.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a869394-7293-4fe3-b868-0006579bd3c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant due to the difference in processing phases. Encoding the input prompt is done in parallel, layer-by-l...</td>\n",
              "      <td>[The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that pro...</td>\n",
              "      <td>The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating ...</td>\n",
              "      <td>[â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from...</td>\n",
              "      <td>Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference sy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>The architecture of Mixtral 8x7B differs from Mistral 7B primarily in its use of Sparse Mixture of Experts (SMoE) layers. While Mistral 7B is a standard transformer model, Mixtral 8x7B incorporate...</td>\n",
              "      <td>[Abstract\\nWe introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all e...</td>\n",
              "      <td>The architecture of Mixtral 8x7B differs from Mistral 7B in terms of feedforward blocks and active parameters used during inference. Mixtral 8x7B has 8 feedforward blocks (experts) in each layer, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>Offloading is used on the A100 server for accelerating MoE-based language models to manage the large model size and limited accelerator memory. The strategy involves using parameter offloading alg...</td>\n",
              "      <td>[# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\\n# Abstract\\nWith the widespread adoption of Large Language Models (LLMs), many deep learning pr...</td>\n",
              "      <td>Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>Mixtral significantly outperforms Llama 2 70B in code benchmarks, as well as in most other categories, while using 5x fewer active parameters during inference. This makes Mixtral not only more eff...</td>\n",
              "      <td>[Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mix...</td>\n",
              "      <td>Mixtral outperforms Llama 2 70B in code benchmarks.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a869394-7293-4fe3-b868-0006579bd3c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a869394-7293-4fe3-b868-0006579bd3c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a869394-7293-4fe3-b868-0006579bd3c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e3efbba6-2453-44e7-932f-a7b5fc8dc67a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3efbba6-2453-44e7-932f-a7b5fc8dc67a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e3efbba6-2453-44e7-932f-a7b5fc8dc67a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?\",\n          \"How does generating tokens affect the inference speed in generative inference?\",\n          \"In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The use of sparse Mixture-of-Experts (MoE) in language models impacts their size by allowing them to have a large number of parameters while maintaining computational efficiency. Sparse MoE models consist of multiple \\\"experts\\\" and a gating function that selects a small subset of these experts for each input, which means that not all parameters are used simultaneously. This results in models that are cheaper to train and perform inference compared to dense models with equivalent perplexity. However, MoE models typically require more parameters than dense models, leading to very large model sizes, often over a trillion parameters. Despite their size, they can achieve near state-of-the-art performance, as seen with models like Mixtral-8x7B. The large size of MoE models can make them difficult to use without high-end hardware, which may limit their adoption compared to dense models.\",\n          \"Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating tokens is done sequentially, where each token depends on the previously generated ones. This sequential process increases computational cost and time, as each additional token requires a forward pass through the model. Consequently, the inference speed is limited by how quickly model parameters can be fetched from memory, making the process slower compared to parallel processing. This impact is particularly pronounced in interactive settings, such as chat applications, where tokens are generated one at a time.\",\n          \"Mixtral outperforms Llama 2 70B on mathematics benchmarks. It achieves superior performance while using 5 times fewer active parameters during inference. This efficiency is due to Mixtral's sparse Mixture-of-Experts model, which uses only 13 billion active parameters per token compared to Llama 2 70B's full parameter set. Overall, Mixtral demonstrates better performance across most categories, including mathematics, compared to Llama 2 70B.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \\\"experts.\\\"\",\n          \"Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference system processes one or few tokens at a time, resulting in a longer waiting time for the next layer's parameters to be loaded. Additionally, the inference speed is limited by how fast parameters can be fetched from system memory. However, by keeping active experts in GPU memory as a cache, the inference time can be sped up if the same experts are activated again in the future. Overall, while caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded.\",\n          \"Mixtral performs better than Llama 2 70B in terms of mathematics benchmarks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', 200) # Adjust display width\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796,
          "referenced_widgets": [
            "9ef41945c6844c31a2cdff909d6f6fef",
            "14ae316d966247bbb0bd38c2493334d2",
            "09036dafd39d494198c4043bfafc7058",
            "50e6a588bfab4e838cb8cd4a60ea8087",
            "eff4b022d651434993018c6192708017",
            "c60df44ab221444c9b12d93546be9db0",
            "c0eeb365c6fe4007ba7b0969fe2cdffd",
            "59fcc11eeffc42d78fceed005f7b9a2c",
            "edc5f8dc563b45d9b676b81dcf3f13a7",
            "09eac4a02dec4b099627f31524af2902",
            "0ca6e6326e4f4f0aa3e85bbbfeea5d17"
          ]
        },
        "id": "sGBS3rApYNIt",
        "outputId": "1a5ac39e-80dc-4804-d86c-d410023b2fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ragas/metrics/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
            "/usr/local/lib/python3.11/dist-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from ragas.metrics._context_entities_recall import (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting RAGAS evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef41945c6844c31a2cdff909d6f6fef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAGAS evaluation complete.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                       question  \\\n",
              "0                                                   What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                How does generating tokens affect the inference speed in generative inference?   \n",
              "2  How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                        When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                   How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "0  The impact of encoding the input prompt on inference speed in generative models is significant due to the difference in processing phases. Encoding the input prompt is done in parallel, layer-by-l...   \n",
              "1  Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating ...   \n",
              "2  The architecture of Mixtral 8x7B differs from Mistral 7B primarily in its use of Sparse Mixture of Experts (SMoE) layers. While Mistral 7B is a standard transformer model, Mixtral 8x7B incorporate...   \n",
              "3  Offloading is used on the A100 server for accelerating MoE-based language models to manage the large model size and limited accelerator memory. The strategy involves using parameter offloading alg...   \n",
              "4  Mixtral significantly outperforms Llama 2 70B in code benchmarks, as well as in most other categories, while using 5x fewer active parameters during inference. This makes Mixtral not only more eff...   \n",
              "\n",
              "                                                                                                                                                                                                  contexts  \\\n",
              "0  [The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that pro...   \n",
              "1  [â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from...   \n",
              "2  [Abstract\\nWe introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all e...   \n",
              "3  [# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\\n# Abstract\\nWith the widespread adoption of Large Language Models (LLMs), many deep learning pr...   \n",
              "4  [Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mix...   \n",
              "\n",
              "                                                                                                                                                                                              ground_truth  \\\n",
              "0  The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively ...   \n",
              "1  Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference sy...   \n",
              "2  The architecture of Mixtral 8x7B differs from Mistral 7B in terms of feedforward blocks and active parameters used during inference. Mixtral 8x7B has 8 feedforward blocks (experts) in each layer, ...   \n",
              "3  Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for r...   \n",
              "4                                                                                                                                                      Mixtral outperforms Llama 2 70B in code benchmarks.   \n",
              "\n",
              "   faithfulness  answer_relevancy  context_precision  context_recall  \\\n",
              "0      1.000000          0.992326           1.000000             1.0   \n",
              "1      1.000000          0.934732           1.000000             0.6   \n",
              "2      0.428571          0.911671           1.000000             1.0   \n",
              "3      0.750000          0.953038           0.755556             0.0   \n",
              "4      1.000000          0.922079           1.000000             1.0   \n",
              "\n",
              "   answer_correctness  answer_similarity  \n",
              "0            0.553538           0.950995  \n",
              "1            0.856394           0.925577  \n",
              "2            0.708698           0.959790  \n",
              "3            0.353497           0.952448  \n",
              "4            0.401302           0.938540  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8f2995f-e6eb-410a-94e3-17111bfa52ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>answer_correctness</th>\n",
              "      <th>answer_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>The impact of encoding the input prompt on inference speed in generative models is significant due to the difference in processing phases. Encoding the input prompt is done in parallel, layer-by-l...</td>\n",
              "      <td>[The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that pro...</td>\n",
              "      <td>The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992326</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.553538</td>\n",
              "      <td>0.950995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating ...</td>\n",
              "      <td>[â¢ The average number of generated tokens outputted by LLMs per query. Much like the assessment of average prompt tokens, this metric provides an evaluation of computational efficiency, but from...</td>\n",
              "      <td>Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference sy...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.934732</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.856394</td>\n",
              "      <td>0.925577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>The architecture of Mixtral 8x7B differs from Mistral 7B primarily in its use of Sparse Mixture of Experts (SMoE) layers. While Mistral 7B is a standard transformer model, Mixtral 8x7B incorporate...</td>\n",
              "      <td>[Abstract\\nWe introduce Mistral 7B, a 7âbillion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all e...</td>\n",
              "      <td>The architecture of Mixtral 8x7B differs from Mistral 7B in terms of feedforward blocks and active parameters used during inference. Mixtral 8x7B has 8 feedforward blocks (experts) in each layer, ...</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.911671</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.708698</td>\n",
              "      <td>0.959790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>Offloading is used on the A100 server for accelerating MoE-based language models to manage the large model size and limited accelerator memory. The strategy involves using parameter offloading alg...</td>\n",
              "      <td>[# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\\n# Abstract\\nWith the widespread adoption of Large Language Models (LLMs), many deep learning pr...</td>\n",
              "      <td>Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for r...</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.953038</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353497</td>\n",
              "      <td>0.952448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>Mixtral significantly outperforms Llama 2 70B in code benchmarks, as well as in most other categories, while using 5x fewer active parameters during inference. This makes Mixtral not only more eff...</td>\n",
              "      <td>[Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\\n70 Mix...</td>\n",
              "      <td>Mixtral outperforms Llama 2 70B in code benchmarks.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.922079</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.401302</td>\n",
              "      <td>0.938540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8f2995f-e6eb-410a-94e3-17111bfa52ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8f2995f-e6eb-410a-94e3-17111bfa52ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8f2995f-e6eb-410a-94e3-17111bfa52ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-84939315-c2c7-4827-8454-d2571c4a8d73\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84939315-c2c7-4827-8454-d2571c4a8d73')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-84939315-c2c7-4827-8454-d2571c4a8d73 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ragas_df",
              "summary": "{\n  \"name\": \"ragas_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?\",\n          \"How does generating tokens affect the inference speed in generative inference?\",\n          \"In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The use of sparse Mixture-of-Experts (MoE) in language models impacts their size by allowing them to have a large number of parameters while maintaining computational efficiency. Sparse MoE models consist of multiple \\\"experts\\\" and a gating function that selects a small subset of these experts for each input, which means that not all parameters are used simultaneously. This results in models that are cheaper to train and perform inference compared to dense models with equivalent perplexity. However, MoE models typically require more parameters than dense models, leading to very large model sizes, often over a trillion parameters. Despite their size, they can achieve near state-of-the-art performance, as seen with models like Mixtral-8x7B. The large size of MoE models can make them difficult to use without high-end hardware, which may limit their adoption compared to dense models.\",\n          \"Generating tokens in generative inference significantly affects inference speed due to the sequential nature of token generation. Unlike the parallel processing of input prompt tokens, generating tokens is done sequentially, where each token depends on the previously generated ones. This sequential process increases computational cost and time, as each additional token requires a forward pass through the model. Consequently, the inference speed is limited by how quickly model parameters can be fetched from memory, making the process slower compared to parallel processing. This impact is particularly pronounced in interactive settings, such as chat applications, where tokens are generated one at a time.\",\n          \"Mixtral outperforms Llama 2 70B on mathematics benchmarks. It achieves superior performance while using 5 times fewer active parameters during inference. This efficiency is due to Mixtral's sparse Mixture-of-Experts model, which uses only 13 billion active parameters per token compared to Llama 2 70B's full parameter set. Overall, Mixtral demonstrates better performance across most categories, including mathematics, compared to Llama 2 70B.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \\\"experts.\\\"\",\n          \"Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference system processes one or few tokens at a time, resulting in a longer waiting time for the next layer's parameters to be loaded. Additionally, the inference speed is limited by how fast parameters can be fetched from system memory. However, by keeping active experts in GPU memory as a cache, the inference time can be sped up if the same experts are activated again in the future. Overall, while caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded.\",\n          \"Mixtral performs better than Llama 2 70B in terms of mathematics benchmarks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20570316455501542,\n        \"min\": 0.42857142857142855,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.42857142857142855,\n          0.6363636363636364,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.034535977251459564,\n        \"min\": 0.9116709702161939,\n        \"max\": 1.0000000000000004,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9999999999999996,\n          0.9347317372488512,\n          0.9364474986625092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07730012058335775,\n        \"min\": 0.7555555555303703,\n        \"max\": 0.99999999998,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.99999999998,\n          0.999999999975,\n          0.7555555555303703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3414023367751831,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6,\n          0.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16377268300520056,\n        \"min\": 0.35349661682256367,\n        \"max\": 0.8563942558679178,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.8237225968003983,\n          0.8563942558679178,\n          0.6612446810347394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016203744923195627,\n        \"min\": 0.9255770234716715,\n        \"max\": 0.976766951347048,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9470643002450709,\n          0.9255770234716715,\n          0.9306930098532438\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from ragas.metrics import (\n",
        "    faithfulness,          # How factual is the answer based on context?\n",
        "    answer_relevancy,      # How relevant is the answer to the question?\n",
        "    context_precision,     # Signal-to-noise ratio in retrieved context\n",
        "    context_recall,        # Were all relevant parts of ground truth context retrieved?\n",
        "    answer_correctness,    # How factually correct is the answer compared to ground truth?\n",
        "    answer_similarity      # How semantically similar is the answer to ground truth?\n",
        ")\n",
        "from ragas import evaluate\n",
        "\n",
        "# Ensure 'ground_truth' column exists and is a string\n",
        "if 'ground_truth' not in df.columns:\n",
        "    raise ValueError(\"DataFrame must contain a 'ground_truth' column for RAGAS evaluation.\")\n",
        "df['ground_truth'] = df['ground_truth'].astype(str)\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset\n",
        "eval_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Define the metrics we want to compute\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall,\n",
        "    answer_correctness,\n",
        "    answer_similarity\n",
        "]\n",
        "\n",
        "print(\"Starting RAGAS evaluation...\")\n",
        "\n",
        "# Run the evaluation\n",
        "# RAGAS will automatically use the OpenAI models set in the environment\n",
        "result = evaluate(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=metrics,\n",
        "    # You can explicitly provide models if needed, but defaults should work\n",
        "    # llm=llm,\n",
        "    # embeddings=embed\n",
        ")\n",
        "\n",
        "print(\"RAGAS evaluation complete.\")\n",
        "\n",
        "# Convert result back to pandas DataFrame for easier viewing\n",
        "ragas_df = result.to_pandas()\n",
        "ragas_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKuXPfTYwTr3"
      },
      "source": [
        "### Interpreting RAGAS Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-pJwPjqwVrU"
      },
      "source": [
        "Let's look at the key metrics produced by RAGAS:\n",
        "\n",
        "#### Retrieval Metrics (Context Related):\n",
        "\n",
        "*   **`context_precision`**: Measures the signal-to-noise ratio of the retrieved contexts. Ideally, all retrieved chunks (`contexts`) should be relevant to the `question`. Scores closer to 1 are better.\n",
        "*   **`context_recall`**: Measures if all necessary information from the `ground_truth` answer was found in the retrieved `contexts`. Scores closer to 1 are better.\n",
        "\n",
        "#### Generation Metrics (Answer Related):\n",
        "\n",
        "*   **`faithfulness`**: Measures how factually consistent the generated `answer` is with the retrieved `contexts`. It checks if the answer hallucinates or makes claims not supported by the provided context. Scores closer to 1 are better.\n",
        "*   **`answer_relevancy`**: Measures how relevant the `answer` is to the original `question`. It penalizes answers that are incomplete or contain redundant information. Scores closer to 1 are better.\n",
        "*   **`answer_similarity`**: Measures the semantic similarity between the generated `answer` and the `ground_truth` answer. Scores closer to 1 mean the generated answer is semantically close to the ideal answer.\n",
        "*   **`answer_correctness`**: Measures both factual correctness (compared to `ground_truth`) and semantic similarity. It's a more holistic measure of whether the answer is right. Scores closer to 1 are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "ZvbZv1IL3uR4",
        "outputId": "a7e8164f-0fe8-4a09-a621-39a8aff38a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Retrieval Metrics ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                question  \\\n",
              "0                                                            What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                         How does generating tokens affect the inference speed in generative inference?   \n",
              "2           How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                                 When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                            How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "5                                                                  In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?   \n",
              "6  What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruc...   \n",
              "7                                          How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?   \n",
              "8                                                       What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?   \n",
              "9                                                                How does LRU caching improve the inference speed of Mixture-of-Experts language models?   \n",
              "\n",
              "   context_precision  context_recall  \n",
              "0           1.000000             1.0  \n",
              "1           1.000000             0.6  \n",
              "2           1.000000             1.0  \n",
              "3           0.755556             0.0  \n",
              "4           1.000000             1.0  \n",
              "5           1.000000             1.0  \n",
              "6           1.000000             1.0  \n",
              "7           1.000000             1.0  \n",
              "8           1.000000             1.0  \n",
              "9           1.000000             0.5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2f2e475-e88e-4cef-ad93-4ae394386db3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does LRU caching improve the inference speed of Mixture-of-Experts language models?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2f2e475-e88e-4cef-ad93-4ae394386db3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a2f2e475-e88e-4cef-ad93-4ae394386db3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a2f2e475-e88e-4cef-ad93-4ae394386db3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b3aa8dfd-0d4a-4873-88a3-e07221721633\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3aa8dfd-0d4a-4873-88a3-e07221721633')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b3aa8dfd-0d4a-4873-88a3-e07221721633 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(ragas_df[['question', 'faithfulness', 'answer_relevancy', 'answer_similarity', 'answer_correctness']])\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?\",\n          \"How does generating tokens affect the inference speed in generative inference?\",\n          \"In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07730012058335775,\n        \"min\": 0.7555555555303703,\n        \"max\": 0.99999999998,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.99999999998,\n          0.999999999975,\n          0.7555555555303703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3414023367751831,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6,\n          0.5,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generation Metrics ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                question  \\\n",
              "0                                                            What is the impact of encoding the input prompt on inference speed in generative inference?   \n",
              "1                                                                         How does generating tokens affect the inference speed in generative inference?   \n",
              "2           How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?   \n",
              "3                                                                 When is offloading used on the A100 server for accelerating MoE-based language models?   \n",
              "4                                                                                            How does Mixtral compare to Llama 2 70B in code benchmarks?   \n",
              "5                                                                  In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?   \n",
              "6  What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruc...   \n",
              "7                                          How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?   \n",
              "8                                                       What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?   \n",
              "9                                                                How does LRU caching improve the inference speed of Mixture-of-Experts language models?   \n",
              "\n",
              "   faithfulness  answer_relevancy  answer_similarity  answer_correctness  \n",
              "0      1.000000          0.992326           0.950995            0.553538  \n",
              "1      1.000000          0.934732           0.925577            0.856394  \n",
              "2      0.428571          0.911671           0.959790            0.708698  \n",
              "3      0.750000          0.953038           0.952448            0.353497  \n",
              "4      1.000000          0.922079           0.938540            0.401302  \n",
              "5      1.000000          0.936447           0.930693            0.661245  \n",
              "6      1.000000          0.997693           0.965707            0.574760  \n",
              "7      1.000000          1.000000           0.976767            0.644192  \n",
              "8      0.636364          1.000000           0.947064            0.823723  \n",
              "9      1.000000          0.952684           0.964203            0.522302  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5fe2b11a-81a2-45bc-8b24-bf1623d46f2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>answer_similarity</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the impact of encoding the input prompt on inference speed in generative inference?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.992326</td>\n",
              "      <td>0.950995</td>\n",
              "      <td>0.553538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does generating tokens affect the inference speed in generative inference?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.934732</td>\n",
              "      <td>0.925577</td>\n",
              "      <td>0.856394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.911671</td>\n",
              "      <td>0.959790</td>\n",
              "      <td>0.708698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>When is offloading used on the A100 server for accelerating MoE-based language models?</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.953038</td>\n",
              "      <td>0.952448</td>\n",
              "      <td>0.353497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does Mixtral compare to Llama 2 70B in code benchmarks?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.922079</td>\n",
              "      <td>0.938540</td>\n",
              "      <td>0.401302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.936447</td>\n",
              "      <td>0.930693</td>\n",
              "      <td>0.661245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruc...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.997693</td>\n",
              "      <td>0.965707</td>\n",
              "      <td>0.574760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976767</td>\n",
              "      <td>0.644192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.947064</td>\n",
              "      <td>0.823723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How does LRU caching improve the inference speed of Mixture-of-Experts language models?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.952684</td>\n",
              "      <td>0.964203</td>\n",
              "      <td>0.522302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fe2b11a-81a2-45bc-8b24-bf1623d46f2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5fe2b11a-81a2-45bc-8b24-bf1623d46f2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5fe2b11a-81a2-45bc-8b24-bf1623d46f2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96937cad-09ef-41b6-9c4a-a9ac2f1ea471\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96937cad-09ef-41b6-9c4a-a9ac2f1ea471')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96937cad-09ef-41b6-9c4a-a9ac2f1ea471 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(ragas_df[['question', 'faithfulness', 'answer_relevancy', 'answer_similarity', 'answer_correctness']])\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?\",\n          \"How does generating tokens affect the inference speed in generative inference?\",\n          \"In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20570316455501542,\n        \"min\": 0.42857142857142855,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.42857142857142855,\n          0.6363636363636364,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.034535977251459564,\n        \"min\": 0.9116709702161939,\n        \"max\": 1.0000000000000004,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9999999999999996,\n          0.9347317372488512,\n          0.9364474986625092\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016203744923195627,\n        \"min\": 0.9255770234716715,\n        \"max\": 0.976766951347048,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9470643002450709,\n          0.9255770234716715,\n          0.9306930098532438\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16377268300520056,\n        \"min\": 0.35349661682256367,\n        \"max\": 0.8563942558679178,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.8237225968003983,\n          0.8563942558679178,\n          0.6612446810347394\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display specific metrics\n",
        "print(\"\\n--- Retrieval Metrics ---\")\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "display(ragas_df[['question', 'context_precision', 'context_recall']])\n",
        "\n",
        "print(\"\\n--- Generation Metrics ---\")\n",
        "display(ragas_df[['question', 'faithfulness', 'answer_relevancy', 'answer_similarity', 'answer_correctness']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA3mHXDO4ALO"
      },
      "source": [
        "Analyze the scores:\n",
        "\n",
        "*   **High `context_recall` and `context_precision`** suggest the retrieval step (ChromaDB + OpenAI embeddings) is working well, finding relevant information without too much noise.\n",
        "*   **High `faithfulness`** indicates the LLM is generating answers based on the provided context and not making things up.\n",
        "*   **High `answer_relevancy`** means the answer directly addresses the question without unnecessary details.\n",
        "*   **High `answer_similarity` and `answer_correctness`** show the final answer is close to the ideal ground truth answer.\n",
        "\n",
        "Low scores in specific areas can pinpoint bottlenecks: poor retrieval, LLM hallucination, or answers that don't properly address the question."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3623a05370914c089a36c3147e53c7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df87f3de1eaf4e74889fcd46382d76ca",
              "IPY_MODEL_99e44f9a57824f798e647e745a192017",
              "IPY_MODEL_faff7d40b97e49bea3ca6d36e282182a"
            ],
            "layout": "IPY_MODEL_31a85ef3a3834ffbb45edbc75a140272"
          }
        },
        "df87f3de1eaf4e74889fcd46382d76ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a48acee8582948009668a91d350094d4",
            "placeholder": "​",
            "style": "IPY_MODEL_711ad01a1b514ca8882c7540dbb585c3",
            "value": "100%"
          }
        },
        "99e44f9a57824f798e647e745a192017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d566e5420347f988d2b06cd9e65c55",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b18655d8e93b4fe3bac404999e850988",
            "value": 50
          }
        },
        "faff7d40b97e49bea3ca6d36e282182a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6fd756b40f4ef59c5524b5f764d20f",
            "placeholder": "​",
            "style": "IPY_MODEL_90d3c212fa7c4941ab2c58d198acdeab",
            "value": " 50/50 [01:27&lt;00:00,  1.74s/it]"
          }
        },
        "31a85ef3a3834ffbb45edbc75a140272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a48acee8582948009668a91d350094d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711ad01a1b514ca8882c7540dbb585c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d566e5420347f988d2b06cd9e65c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18655d8e93b4fe3bac404999e850988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf6fd756b40f4ef59c5524b5f764d20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d3c212fa7c4941ab2c58d198acdeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdb5700136604edbb41a38f12d1bec63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_271b2957ee434acb854a06a83eaf5774",
              "IPY_MODEL_08e3bd7f9c6d45e0aebc27b6991885c5",
              "IPY_MODEL_ef08c2d004c74161859d9f0dfa35fc2a"
            ],
            "layout": "IPY_MODEL_48d65d19d08e4daaba34a9e1b36e43a4"
          }
        },
        "271b2957ee434acb854a06a83eaf5774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc87c8b8386458cb2439b1a8ee062f6",
            "placeholder": "​",
            "style": "IPY_MODEL_c1f752c6be944633892fcfdad69dfb33",
            "value": "Downloading data: 100%"
          }
        },
        "08e3bd7f9c6d45e0aebc27b6991885c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc75b37ff4748a1aa5e88acd338d9f3",
            "max": 86995,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a4f061dd14046188b1ec72f1b74e0a1",
            "value": 86995
          }
        },
        "ef08c2d004c74161859d9f0dfa35fc2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee2ef68d735f4223aad2a06da4c7e570",
            "placeholder": "​",
            "style": "IPY_MODEL_3c15f9da62e341c58878cb5911db1a02",
            "value": " 87.0k/87.0k [00:00&lt;00:00, 772kB/s]"
          }
        },
        "48d65d19d08e4daaba34a9e1b36e43a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dc87c8b8386458cb2439b1a8ee062f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f752c6be944633892fcfdad69dfb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc75b37ff4748a1aa5e88acd338d9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a4f061dd14046188b1ec72f1b74e0a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee2ef68d735f4223aad2a06da4c7e570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c15f9da62e341c58878cb5911db1a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d72f9242f54a44f6a1b9414ae1803164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e417f4be46e84539bfd0ac3d69c99f14",
              "IPY_MODEL_8abfb84c3c004f27b6b8882da70c95b2",
              "IPY_MODEL_ae362487109c4859ae84adf133c74286"
            ],
            "layout": "IPY_MODEL_65f2330c47b74ce08cb9495afe5750e8"
          }
        },
        "e417f4be46e84539bfd0ac3d69c99f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b4679f227a405fb6da3e684a254d8a",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ce5d063a814981bd5e46c4fde742c5",
            "value": "Generating train split: 100%"
          }
        },
        "8abfb84c3c004f27b6b8882da70c95b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0f869c3677f44f79262fb6370e55f54",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec90b8dd36764eb3baa580089e57eeb4",
            "value": 51
          }
        },
        "ae362487109c4859ae84adf133c74286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6012890e8fab4fa1884095fdd77d6334",
            "placeholder": "​",
            "style": "IPY_MODEL_6e9ac41d57084446ae1c364ac12bf1d7",
            "value": " 51/51 [00:00&lt;00:00, 2444.43 examples/s]"
          }
        },
        "65f2330c47b74ce08cb9495afe5750e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b4679f227a405fb6da3e684a254d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ce5d063a814981bd5e46c4fde742c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0f869c3677f44f79262fb6370e55f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec90b8dd36764eb3baa580089e57eeb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6012890e8fab4fa1884095fdd77d6334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e9ac41d57084446ae1c364ac12bf1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2252b8845604422883461382ffade0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0e179f760d842ebb0f6d25bd9727e3d",
              "IPY_MODEL_9d704bd907474277aed5eb9924d843e4",
              "IPY_MODEL_2b6e2621bb6a42acb091ba16815dce7a"
            ],
            "layout": "IPY_MODEL_cd6e575240d74588b88966eb1d546aa2"
          }
        },
        "c0e179f760d842ebb0f6d25bd9727e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c06219f55b5d4656bcadf081c42373b2",
            "placeholder": "​",
            "style": "IPY_MODEL_068c25497bbd42d0a604c86b9744c03b",
            "value": "100%"
          }
        },
        "9d704bd907474277aed5eb9924d843e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7acd4a6ad3a7442ea4f319146ed0da99",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bba2f654afc74096bc03711db8e6244d",
            "value": 10
          }
        },
        "2b6e2621bb6a42acb091ba16815dce7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c181b9310fa4ef39a999d53bb44cd33",
            "placeholder": "​",
            "style": "IPY_MODEL_6f230e7d6d33472ba9666edcccdc1c5c",
            "value": " 10/10 [00:50&lt;00:00,  5.44s/it]"
          }
        },
        "cd6e575240d74588b88966eb1d546aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c06219f55b5d4656bcadf081c42373b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068c25497bbd42d0a604c86b9744c03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7acd4a6ad3a7442ea4f319146ed0da99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba2f654afc74096bc03711db8e6244d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c181b9310fa4ef39a999d53bb44cd33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f230e7d6d33472ba9666edcccdc1c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ef41945c6844c31a2cdff909d6f6fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14ae316d966247bbb0bd38c2493334d2",
              "IPY_MODEL_09036dafd39d494198c4043bfafc7058",
              "IPY_MODEL_50e6a588bfab4e838cb8cd4a60ea8087"
            ],
            "layout": "IPY_MODEL_eff4b022d651434993018c6192708017"
          }
        },
        "14ae316d966247bbb0bd38c2493334d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60df44ab221444c9b12d93546be9db0",
            "placeholder": "​",
            "style": "IPY_MODEL_c0eeb365c6fe4007ba7b0969fe2cdffd",
            "value": "Evaluating: 100%"
          }
        },
        "09036dafd39d494198c4043bfafc7058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59fcc11eeffc42d78fceed005f7b9a2c",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edc5f8dc563b45d9b676b81dcf3f13a7",
            "value": 60
          }
        },
        "50e6a588bfab4e838cb8cd4a60ea8087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09eac4a02dec4b099627f31524af2902",
            "placeholder": "​",
            "style": "IPY_MODEL_0ca6e6326e4f4f0aa3e85bbbfeea5d17",
            "value": " 60/60 [00:23&lt;00:00,  1.68it/s]"
          }
        },
        "eff4b022d651434993018c6192708017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60df44ab221444c9b12d93546be9db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0eeb365c6fe4007ba7b0969fe2cdffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59fcc11eeffc42d78fceed005f7b9a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edc5f8dc563b45d9b676b81dcf3f13a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09eac4a02dec4b099627f31524af2902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca6e6326e4f4f0aa3e85bbbfeea5d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}